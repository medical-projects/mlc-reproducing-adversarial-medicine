= Blind Adversarial Attacks Against Medical Deep Learning Systems
:imagesdir: images
:icons: font
:date: May 16, 2019
:my_name: Peter Steinbach
:my_email: steinbach@extern.mpi-cbg.de
:my_twitter: psteinb_
:my_github: psteinb
:revealjs_slideNumber: true
:revealjs_center: false
:revealjs_BackgroundVertical: null
:revealjs_width: 1600
:revealjs_margin: .05
:revealjs_plugin_pdf: enabled #you run your presentation in a browser with ?print-pdf at the end of the URL, you can then use the default print function to print the slide deck into a PDF document.
:customcss: custom.css
:stem:

mailto:{my_email}[{my_name}] +
https://twitter.com/{my_twitter}[icon:twitter[] psteinb_] https://github.com/{my_github}[icon:github[] psteinb] + 
2nd MLC Workshop {date} +
Helmholtz-Zentrum Dresden-Rossendorf, Germany

== Introducing Adversarial Examples

icon:question-circle[5x]

=== Supervised Deep Learning Classifiers

image:dl-process.png[heigth=80%]

From <<MapRBlog>>

[.notes]
--
* classification (accuracy better than human)
* large training/test set
* data set = good balance of classes
--

=== What are Adversarial Examples?

image:advex-explained-1-nogradient.png[width=90%]

From <<Goodfellow14>> (based on <<Szegedy14>>)

=== Fooling by printing

image::advex-explained-2.png[height=140%]

From <<Kurakin16>>

=== Fooling by backdoors

image:badnet-without-caption.png[width=50%]

From <<GU>>


=== Consequences?

- security of autonomous driving
- security of voice recognition systems (Siri, Alexa, ...)
- product quality of online tools
- trust in automated decision tools
- trust in scientific classifiers

WARNING: Affects any publicly deployed ML system that allows unsupervised usage!


=== Taxonomy based on <<Biggio18>>

[cols="4*",options="noheader"]
|===
|type
|white box
|grey box
|black box

|adversary knows
|all of
|subset of
|only queries

a| &nbsp;
a| 
* dataset
* feature set
* learning alg + loss
* trained weights
a| 
* [.graytext]#dataset#
* feature set
* learning alg + loss
* [.graytext]#trained weights#
a| 
* [.graytext]#dataset#
* [.graytext]#feature set#
* [.graytext]#learning alg + loss#
* [.graytext]#trained weights#
|===

WARNING: Boundaries not clear cut


== Adversarial Attacks Against Medical Deep Learning Systems

=== Doctor's Appointment 

icon:user-md[5x]

=== Measurement

icon:heartbeat[5x]

=== Diagnosis

icon:medkit[5x] &nbsp; or &nbsp; icon:heart[5x]

=== Paper

"Adversarial Attacks Against Medical Deep Learning Systems" +
Samuel G. Finlayson, Hyung Won Chung, Isaac S. Kohane, Andrew L. Beam (HMS+Harvard+MIT)

Science  22 Mar 2019:
Vol. 363, Issue 6433, pp. 1287-1289
DOI: https://doi.org/10.1126/science.aaw4399[10.1126/science.aaw4399]

[NOTE.speaker]
--
- published in Science
- reproduced it
--

=== 3 open datasets

- https://www.kaggle.com/c/diabetic-retinopathy-detection/data[Diabetic Retinopathy Detection]
- http://academictorrents.com/details/557481faacd824c83fbf57dcf7b6da9383b3235a[X-ray Dataset of 14 Common Thorax Disease]
- https://www.isic-archive.com/#!/topWithHeader/onlyHeaderTop/gallery[skin cancer dataset] (omitted from this presentation)

=== Data Access

- 83 GB shared through kaggle (Diabetic Retinopathy Detection)
- 42 GB shared through science torrent (X-ray Dataset of 14 Common Thorax Disease)

IMPORTANT: Very Good!

[.notes]
--
* download of datasets was somewhat easy
* kaggle required to create an account
* omitted dataset used custom formatted REST API without front-end library :/
--

=== Diabetic Retinopathy (DR)

[cols=">.^35,<.^65"]
|===
a|
image:dr-original.png[width=80%,align=right]

a|
* https://www.kaggle.com/c/diabetic-retinopathy-detection/data[kaggle dataset]: https://en.wikipedia.org/wiki/Ophthalmoscopy[fundus photographs]
* Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world.
* Clinicians can identify DR by the presence of lesions (vascular abnormalities due to disease)
* 88702 images, approx. 4752x3168 RGB jpegs
* 5 labels (severity of DR)

|===



=== http://academictorrents.com/details/557481faacd824c83fbf57dcf7b6da9383b3235a[ChestX-ray8]

[cols=">.^35,<.^65"]
|===
a|
image:cxr-original.png[width=80%,align=right]

a|
* Classification and Localization of Common Thorax Diseases
* 112,120 images
* 1024x1024 8-bit greyscale
* 14 labels, appearing non-exclusively

|===


=== Code

- https://github.com/sgfin/adversarial-medicine[github repo]
- training weights shared as .npy files through dropbox
- notebooks reproduce all figures based on downloaded weights
- python script to retrain two architectures (https://keras.io/applications/#resnet[Resnet50], https://keras.io/applications/#inceptionv3[InceptionV3])

=== Code

- code was executable once dropbox weights downloaded
- notebooks work from top to bottom
- code largely undocumented
- paper reports Resnet50 results, weights are for InceptionV3
- readme reports similar results for either

IMPORTANT: Ok!

== My Rebuttal



=== White Box Attacks



=== Attacks always assume knowledge

Paper defines Black Box attacks as:


=== Pure Black-box on Diabetic Retinopathy

[cols="^.<,^.<,^.<",width=100%,frame=none,grid=none]
|===
a| image:dr-original.png[width=80%]
a| image:dr-adversarial.png[width=80%]
a| image:dr-diff.png[width=80%]
|===

=== Library Support: Foolbox



=== Pure Black-box on Chest X-Ray

[cols="^.<,^.<,^.<",width=100%,frame=none,grid=none]
|===
a| image:cxr-original.png[width=80%]
a| image:cxr-adversarial.png[width=80%]
a| image:cxr-diff.png[width=80%]
|===

== Summary

include::references.adoc[]

